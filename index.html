<!DOCTYPE html>
<html lang="gu">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Vidushi AI (Auto)</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        /* --- BASIC SETUP --- */
        body, html { margin: 0; padding: 0; width: 100%; height: 100%; overflow: hidden; background-color: black; font-family: 'Segoe UI', sans-serif; }

        /* --- VIDEO LAYER --- */
        #video-container { position: fixed; top: 0; left: 0; width: 100%; height: 100%; z-index: 1; }
        video { width: 100%; height: 100%; object-fit: cover; position: absolute; top: 0; left: 0; display: none; }
        #silent-video { display: block; }

        /* --- UI LAYER --- */
        #top-bar { position: fixed; top: 0; left: 0; width: 100%; padding: 15px 0; background: rgba(0, 0, 0, 0.5); color: white; text-align: center; font-size: 24px; font-weight: bold; z-index: 10; text-shadow: 1px 1px 2px black; letter-spacing: 2px; }

        /* --- CONTROLS --- */
        #mic-container { position: fixed; bottom: 50px; left: 50%; transform: translateX(-50%); z-index: 20; display: flex; flex-direction: column; align-items: center; width: 90%; }
        #status-text { color: white; margin-bottom: 15px; font-size: 14px; font-weight: bold; text-shadow: 1px 1px 2px black; background: rgba(0,0,0,0.6); padding: 8px 15px; border-radius: 12px; text-align: center; max-width: 90%; word-wrap: break-word; }
        .controls-row { display: flex; align-items: center; gap: 20px; }
        
        #mic-btn { width: 80px; height: 80px; border-radius: 50%; background: #ff4500; border: 4px solid white; color: white; font-size: 35px; cursor: pointer; box-shadow: 0 4px 20px rgba(0,0,0,0.6); display: flex; align-items: center; justify-content: center; transition: transform 0.2s; }
        #mic-btn:active { transform: scale(0.95); }
        .listening { animation: pulse 1.5s infinite; background: red !important; }

        #stop-btn { width: 50px; height: 50px; border-radius: 50%; background: rgba(100, 100, 100, 0.8); border: 2px solid white; color: white; font-size: 20px; cursor: pointer; display: flex; align-items: center; justify-content: center; }
        #stop-btn:active { background: red; }

        @keyframes pulse { 0% { box-shadow: 0 0 0 0 rgba(255, 0, 0, 0.7); } 70% { box-shadow: 0 0 0 20px rgba(255, 0, 0, 0); } 100% { box-shadow: 0 0 0 0 rgba(255, 0, 0, 0); } }

        /* Image Overlay */
        #image-overlay { display: none; position: fixed; top: 50%; left: 50%; transform: translate(-50%, -50%); z-index: 50; background: rgba(0,0,0,0.8); padding: 10px; border-radius: 15px; border: 2px solid orange; box-shadow: 0 0 30px rgba(0,0,0,0.8); }
        #generated-image { max-width: 90vw; max-height: 60vh; border-radius: 10px; }
        #close-img-btn { position: absolute; top: -15px; right: -15px; background: red; color: white; border: none; border-radius: 50%; width: 30px; height: 30px; cursor: pointer; font-weight: bold; }

        /* Clear Memory Button (Small, Top Right) */
        #clear-mem-btn { position: fixed; top: 15px; right: 15px; z-index: 20; background: #333; color: white; border: 1px solid #555; padding: 5px 10px; border-radius: 5px; cursor: pointer; font-size: 12px; }
    </style>
</head>
<body>

    <div id="video-container">
        <video id="silent-video" autoplay muted loop playsinline><source src="silent.mp4" type="video/mp4"></video>
        <video id="talking-video" muted loop playsinline><source src="talking.mp4" type="video/mp4"></video>
        <video id="thinking-video" muted loop playsinline><source src="thinking.mp4" type="video/mp4"></video>
        <video id="smiling-video" muted loop playsinline><source src="smiling.mp4" type="video/mp4"></video>
    </div>

    <div id="top-bar">àªµàª¿àª¦à«àª·à«€</div>
    <button id="clear-mem-btn" onclick="clearMemory()">ğŸ§¹ Reset</button>

    <div id="image-overlay">
        <button id="close-img-btn" onclick="closeImage()">X</button>
        <img id="generated-image" src="" alt="Generated Image">
    </div>

    <div id="mic-container">
        <div id="status-text">àªµàª¾àª¤ àª•àª°àªµàª¾ àª®àª¾àªˆàª• àª¦àª¬àª¾àªµà«‹</div>
        <div class="controls-row">
            <button id="mic-btn" onclick="startConversation()"><i class="fas fa-microphone"></i></button>
            <button id="stop-btn" onclick="stopConversation()"><i class="fas fa-stop"></i></button>
        </div>
    </div>

    <script>
        // ==========================================
        //  àª¤àª®àª¾àª°à«€ API KEYS àª…àª¹à«€àª‚ àªªà«‡àª¸à«àªŸ àª•àª°à«‹ (Paste Here)
        // ==========================================
        
        const GROQ_API_KEY = "YOUR_GROQ_KEY_HERE";   // <-- àª…àª¹à«€àª‚ àª¤àª®àª¾àª°à«€ gsk_... àªµàª¾àª³à«€ Groq àª•à«€ àª®à«‚àª•à«‹
        const ELEVEN_API_KEY = "YOUR_ELEVEN_KEY_HERE"; // <-- àª…àª¹à«€àª‚ àª¤àª®àª¾àª°à«€ ElevenLabs àª•à«€ àª®à«‚àª•à«‹
        
        // ==========================================

        const VOICE_ID = "21m00Tcm4TlvDq8ikWAM"; // Rachel Voice (Default)
        const MODEL = "llama-3.3-70b-versatile"; // Best Model

        // --- ELEMENTS ---
        const silentVideo = document.getElementById('silent-video');
        const talkingVideo = document.getElementById('talking-video');
        const thinkingVideo = document.getElementById('thinking-video');
        const smilingVideo = document.getElementById('smiling-video');
        const micBtn = document.getElementById('mic-btn');
        const statusText = document.getElementById('status-text');
        const imageOverlay = document.getElementById('image-overlay');
        const generatedImage = document.getElementById('generated-image');

        // Variables
        let isConversationActive = false; 
        let wakeLock = null;
        let chatHistory = JSON.parse(localStorage.getItem('vidushi_memory')) || [];
        let currentAudio = null;

        // --- VIDEO CONTROL ---
        function switchVideo(state) {
            const videos = [silentVideo, talkingVideo, thinkingVideo, smilingVideo];
            videos.forEach(v => { v.style.display = 'none'; v.pause(); });

            let activeVideo;
            if (state === 'talking') activeVideo = talkingVideo;
            else if (state === 'thinking') activeVideo = thinkingVideo;
            else if (state === 'smiling') activeVideo = smilingVideo;
            else activeVideo = silentVideo;

            activeVideo.style.display = 'block';
            activeVideo.play().catch(e => {
                if (state !== 'silent') { silentVideo.style.display = 'block'; silentVideo.play().catch(e=>{}); }
            });
        }

        // --- MEMORY ---
        function clearMemory() {
            chatHistory = [];
            localStorage.removeItem('vidushi_memory');
            alert("àª¯àª¾àª¦àª¶àª•à«àª¤àª¿ àª­à«‚àª‚àª¸àª¾àªˆ àª—àªˆ!");
        }

        // --- WAKE LOCK ---
        async function requestWakeLock() { try { if ('wakeLock' in navigator) wakeLock = await navigator.wakeLock.request('screen'); } catch (err) {} }
        function releaseWakeLock() { if (wakeLock) { wakeLock.release(); wakeLock = null; } }

        // --- SPEECH RECOGNITION ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;

        if (SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.lang = 'gu-IN';
            recognition.continuous = false;

            recognition.onstart = () => { 
                micBtn.classList.add('listening'); statusText.innerText = "àª¸àª¾àª‚àª­àª³à«àª‚ àª›à«àª‚..."; switchVideo('silent');
            };
            recognition.onend = () => micBtn.classList.remove('listening');
            recognition.onresult = (e) => {
                let text = e.results[0][0].transcript;
                text = text.replace(/àªµàª¿àª¦à«‡àª¶à«€/gi, "àªµàª¿àª¦à«àª·à«€").replace(/àªµàª¿àª¦à«‚àª·à«€/gi, "àªµàª¿àª¦à«àª·à«€");
                statusText.innerText = "àªµàª¿àªšàª¾àª°à«àª‚ àª›à«àª‚...";
                askGroq(text);
            };
        } else {
            statusText.innerText = "Mic Not Supported";
        }

        function startConversation() {
            if(!GROQ_API_KEY.startsWith("gsk_")) { alert("àª•à«‹àª¡àª®àª¾àª‚ Groq API Key àª¨àª¾àª–àªµàª¾àª¨à«€ àª¬àª¾àª•à«€ àª›à«‡!"); return; }
            if (currentAudio) { currentAudio.pause(); currentAudio = null; }
            if (window.speechSynthesis.speaking) window.speechSynthesis.cancel();
            
            isConversationActive = true; 
            requestWakeLock();
            recognition.start();
        }

        function stopConversation() {
            isConversationActive = false;
            if (currentAudio) { currentAudio.pause(); }
            window.speechSynthesis.cancel();
            recognition.stop();
            switchVideo('silent');
            releaseWakeLock();
            statusText.innerText = "àª¬àª‚àª§";
        }

        // --- GROQ API CALL ---
        async function askGroq(userText) {
            switchVideo('thinking');

            if (userText.toLowerCase().includes("youtube")) { window.open("https://youtube.com", "_blank"); speak("YouTube àª–à«‹àª²à«€ àª°àª¹à«€ àª›à«àª‚."); return; }
            if (userText.toLowerCase().includes("google")) { window.open("https://google.com", "_blank"); speak("Google àª–à«‹àª²à«€ àª°àª¹à«€ àª›à«àª‚."); return; }

            const now = new Date();
            const timeString = now.toLocaleTimeString('gu-IN', { hour: '2-digit', minute: '2-digit' });
            
            const systemPrompt = `
                àª¤àª¾àª°à«àª‚ àª¨àª¾àª® 'àªµàª¿àª¦à«àª·à«€' àª›à«‡. àª¤à«àª‚ 'àª¦à«‡àªµà«‡àª¨à«àª¦à«àª°àª­àª¾àªˆ' àª¦à«àªµàª¾àª°àª¾ àª¨àª¿àª°à«àª®àª¿àª¤ AI àª›à«‡.
                àªµàª°à«àª¤àª®àª¾àª¨ àª¸àª®àª¯: ${timeString}.
                àª¨àª¿àª¯àª®à«‹:
                1. àª¹àª‚àª®à«‡àª¶àª¾ àª¸à«àª¤à«àª°à«€àª²àª¿àª‚àª—àª®àª¾àª‚ àªµàª¾àª¤ àª•àª°àªµà«€.
                2. "àªœàª¯ àª¶à«àª°à«€ àª•à«ƒàª·à«àª£" àª¥à«€ àª¶àª°à«‚àª†àª¤ àª•àª°àªµà«€ (àªàª•àªµàª¾àª°).
                3. àªœàªµàª¾àª¬ àªŸà«‚àª‚àª•àª¾ àª…àª¨à«‡ àª®à«àª¦à«àª¦àª¾àª¸àª° àª†àªªàªµàª¾.
                4. àª¸àª¨àª¾àª¤àª¨ àª§àª°à«àª® àªµàª¿àª¶à«‡ àªªà«‚àª›à«‡ àª¤à«‹ àªœ àªµàª¿àª—àª¤à«‡ àª•àª¹à«‡àªµà«àª‚.
                5. àªšàª¿àª¤à«àª° àª®àª¾àªŸà«‡: [IMAGE: english prompt].
            `;

            let messagesToSend = [{ role: "system", content: systemPrompt }, ...chatHistory.slice(-10)];
            messagesToSend.push({ role: "user", content: userText });

            try {
                const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
                    method: "POST",
                    headers: { "Content-Type": "application/json", "Authorization": `Bearer ${GROQ_API_KEY}` },
                    body: JSON.stringify({ model: MODEL, messages: messagesToSend, max_tokens: 300 })
                });

                const data = await response.json();

                if (data.choices && data.choices[0]) {
                    const reply = data.choices[0].message.content;
                    chatHistory.push({ role: "user", content: userText });
                    chatHistory.push({ role: "assistant", content: reply });
                    localStorage.setItem('vidushi_memory', JSON.stringify(chatHistory));

                    if (reply.includes("[IMAGE:")) {
                        const imgPrompt = reply.match(/\[IMAGE: (.*?)\]/)[1];
                        showImage(imgPrompt);
                        const speakText = reply.replace(/\[IMAGE:.*?\]/, "àª† àª°àª¹à«àª¯à«àª‚ àªšàª¿àª¤à«àª°.");
                        speak(speakText);
                    } else {
                        speak(reply);
                    }
                }
            } catch (error) {
                console.error(error);
                statusText.innerText = "Net Error";
                speak("àª‡àª¨à«àªŸàª°àª¨à«‡àªŸ àª•àª¨à«‡àª•à«àª¶àª¨ àªšà«‡àª• àª•àª°à«‹.");
                isConversationActive = false;
            }
        }

        // --- IMAGE ---
        function showImage(prompt) {
            generatedImage.src = `https://image.pollinations.ai/prompt/${encodeURIComponent(prompt)}`;
            imageOverlay.style.display = 'block';
        }
        function closeImage() { imageOverlay.style.display = 'none'; }

        // --- SPEAK LOGIC (ElevenLabs First) ---
        async function speak(text) {
            let cleanText = text.replace(/[*#_]/g, "").replace(/\[IMAGE:.*?\]/g, "").replace(/àª–à«‚àª¬\s+àªœ/g, "àª–à«àª¬àªœ");
            
            const happyWords = ["àª†àª­àª¾àª°", "àª®àªœàª¾", "àª¸àª°àª¸", "àª—àª®à«àª¯à«àª‚", "àª¹àª¸à«€", "ğŸ˜Š", "ğŸ˜‚", "àª–à«àª¶à«€", "àªœàª¯ àª¶à«àª°à«€ àª•à«ƒàª·à«àª£"];
            let isHappy = happyWords.some(word => cleanText.includes(word));

            // 1. Try ElevenLabs
            if (ELEVEN_API_KEY && !ELEVEN_API_KEY.includes("YOUR_ELEVEN")) {
                statusText.innerText = "ElevenLabs àª¬à«‹àª²à«‡ àª›à«‡...";
                try {
                    const response = await fetch(`https://api.elevenlabs.io/v1/text-to-speech/${VOICE_ID}`, {
                        method: "POST",
                        headers: { "xi-api-key": ELEVEN_API_KEY, "Content-Type": "application/json" },
                        body: JSON.stringify({
                            text: cleanText,
                            model_id: "eleven_multilingual_v2",
                            voice_settings: { stability: 0.5, similarity_boost: 0.75 }
                        })
                    });

                    if (response.ok) {
                        const blob = await response.blob();
                        const url = URL.createObjectURL(blob);
                        currentAudio = new Audio(url);
                        currentAudio.onplay = () => switchVideo('talking');
                        currentAudio.onended = () => finishSpeaking(isHappy);
                        currentAudio.play();
                        return;
                    }
                } catch (e) { console.log("ElevenLabs Fail, using Google."); }
            }

            // 2. Fallback Google TTS
            statusText.innerText = "Google àª¬à«‹àª²à«‡ àª›à«‡...";
            const utterance = new SpeechSynthesisUtterance(cleanText);
            utterance.lang = 'gu-IN';
            utterance.onstart = () => switchVideo('talking');
            utterance.onend = () => finishSpeaking(isHappy);
            window.speechSynthesis.speak(utterance);
        }

        function finishSpeaking(isHappy) {
            if (isHappy) {
                switchVideo('smiling');
                setTimeout(handleLoop, 3000);
            } else {
                handleLoop();
            }
        }

        function handleLoop() {
            switchVideo('silent');
            if (isConversationActive) {
                statusText.innerText = "àª«àª°à«€ àª¸àª¾àª‚àª­àª³à«àª‚ àª›à«àª‚...";
                setTimeout(() => { try { recognition.start(); } catch(e) {} }, 500);
            } else {
                releaseWakeLock();
            }
        }
    </script>
</body>
</html>
